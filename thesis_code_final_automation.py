# -*- coding: utf-8 -*-
"""Thesis Code Final Automation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11EPGDkGbQmb1hFZyXspAJQvCny8oPJlP
"""

from google.colab import drive
drive.mount('/content/drive')

import subprocess
import sys

def install_package(package):
    subprocess.check_call([sys.executable, "-m", "pip", "install", package])

def check_and_install_requirements(requirements_file):
    with open(requirements_file, 'r') as file:
        requirements = file.readlines()

    for requirement in requirements:
        package = requirement.strip()
        try:
            __import__(package)
            print(f"{package} is already installed.")
        except ImportError:
            print(f"{package} is not installed. Installing...")
            install_package(package)

check_and_install_requirements('/content/requirements.txt')

# installin reportLab whichh would allow export of each server file post initial analysis
import os
import gzip
import warnings
from datetime import datetime
from PIL import Image
import glob
import subprocess
import sys

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import io

from pdf_creator import create_pdf, PDFSection, PDFContent

from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.decomposition import PCA

from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.stattools import adfuller, kpss, acf, pacf
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.holtwinters import SimpleExpSmoothing
from statsmodels.stats.diagnostic import acorr_ljungbox

from pmdarima import auto_arima
from pmdarima.model_selection import cross_val_score
from pmdarima.metrics import smape

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

from pandas.plotting import lag_plot

from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
from reportlab.lib.utils import ImageReader
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle
from reportlab.lib import colors

# Suppress warnings (optional)
warnings.filterwarnings('ignore', category=UserWarning, append=True)

"""## before running copy banner.jpg, pdf_creator.py and requirements.txt"""

def complete_analysis_function(file_path):

        # Initialising the PDF section for Feature Engineering

        section1 = PDFSection("Feature Engineering")
        df=pd.read_csv(file_path)

        # Extracting directory and filename
        directory, filename = os.path.split(file_path)

        # Extracting base filename without extension
        base_filename = os.path.splitext(filename)[0]

        # Splitting the base filename into location_name and server_name
        location_name, server_name = base_filename.split('-')

        # Printing the extracted parts
        # print("Directory:", directory)
        # print("Filename:", filename)
        # print("Base Filename:", base_filename)
        # print("Location Name:", location_name)
        # print("Server Name:", server_name)
        location_map = {
                'GLCHBS': 'Switzerland',
                'GLCHST': 'Austria'
            }

        location_country = location_map.get(location_name)
        # print(f"Server Location: {location_country}")
        if location_country is None:
            raise ValueError(f"Invalid location name: {location_name}")

        plots_folder = os.path.join(directory,  f"{base_filename}/Plots")
        if not os.path.exists(plots_folder):
            os.makedirs(plots_folder)
        output_folder = os.path.join(directory, f"{base_filename}/Output")
        if not os.path.exists(output_folder):
            os.makedirs(output_folder)

        plots_folder1 = os.path.join(directory, f'{base_filename}/Plots/Untreated_data')
        output_folder1 = os.path.join(directory, f'{base_filename}/Output/Untreated_data')
        plots_folder2= os.path.join(directory, f'{base_filename}/Plots/Lag_data')
        output_folder2= os.path.join(directory, f'{base_filename}/Output/Lag_data')
        plots_folder3 = os.path.join(directory, f'{base_filename}/Plots/PCA_data')
        output_folder3 = os.path.join(directory, f'{base_filename}/Output/PCA_data')
        plots_folder4 = os.path.join(directory, f'{base_filename}/Plots/lag_pca_data')
        output_folder4 = os.path.join(directory, f'{base_filename}/Output/lag_pca_data')

        if not os.path.exists(plots_folder1):
            os.makedirs(plots_folder1)

        if not os.path.exists(output_folder1):
            os.makedirs(output_folder1)

        if not os.path.exists(plots_folder2):
            os.makedirs(plots_folder2)

        if not os.path.exists(output_folder2):
            os.makedirs(output_folder2)

        if not os.path.exists(plots_folder3):
            os.makedirs(plots_folder3)

        if not os.path.exists(output_folder3):
            os.makedirs(output_folder3)

        if not os.path.exists(plots_folder4):
            os.makedirs(plots_folder4)

        if not os.path.exists(output_folder4):
            os.makedirs(output_folder4)

        # Writing the server information to the pdf
        section1.add_content("Server information", f"Filename: {filename}")
        section1.add_content("Server information", f"Base Filename: {base_filename} \n Location Name: {location_name} ")
        section1.add_content("Server information", f"Server Name: {server_name}")
        section1.add_content("Server information", f"Server Location: {location_country}")

        # Define the file path and content
        banner_path = f"{directory}/banner.jpg"
        title = f"Comprehensive Analysis of Server {base_filename}"

        df['Time_Of_Day_Seconds'] = pd.to_datetime(df['Time_Of_Day_Seconds'])

        # 1 to be replaced with total number of observations
        df['PkgWatt'] = df['PkgWatt'] *((0.0277)*(1/100000))

        df['Core'] = df['Core'].apply(lambda x: round(x, 2))
        df['CPU'] = df['CPU'].apply(lambda x: round(x, 2))
        df['Avg_MHz'] = df['Avg_MHz'].apply(lambda x: round(x, 2))
        df['Busy%'] = df['Busy%'].apply(lambda x: round(x, 2))
        df['Bzy_MHz'] = df['Bzy_MHz'].apply(lambda x: round(x, 2))

        df_head=df.iloc[:,[0,1,2,3,4]].head()
        section1.add_content("\n Initial combined aggregation at seconds",df_head)

        df_head=df.iloc[:,[0,5,6]].head()
        section1.add_content("\n Initial combined aggregation at seconds",df_head)

        # PkgWatt time series aggregated on hour level-Check1-Okay
        agg_dict = {col: 'mean' for col in df.columns if col != 'PkgWatt' and col != 'Time_Of_Day_Seconds'}
        agg_dict['PkgWatt'] = 'sum'
        df_hourly = df.groupby(pd.Grouper(key='Time_Of_Day_Seconds', freq='H')).agg(agg_dict).reset_index()

        # Create fig
        plt.figure(figsize=(12, 6))
        sns.lineplot(x=df_hourly['Time_Of_Day_Seconds'], y=df_hourly['PkgWatt'])

        # x axis formatin
        plt.xticks(rotation=45, ha='right')

        # setin the title
        plt.title(f'{base_filename} kWatt Time Series (Hourly Aggregation)')
        plt.xlabel("Time")
        plt.ylabel("kWatts used")
        plt.tight_layout()

        # switching ticks
        plt.minorticks_on()

        # grid
        plt.grid(which='both', linestyle='--', linewidth=0.5)
        plt.grid(which='major', color='black', linestyle='-', linewidth=0.7)
        plt.grid(which='minor', color='gray', linestyle=':', linewidth=0.5)

        plt.savefig(f'{plots_folder}/{base_filename}_plot1.png')
        section1.add_content(f"\n {base_filename} Hour Aggregated : kWattage","Wattage mapped over time",image_path=f'{plots_folder}/{base_filename}_plot1.png')
        # plt.show()

        # PkgWatt time series aggregated on day level-C2
        agg_dict = {col: 'mean' for col in df.columns if col != 'PkgWatt' and col != 'Time_Of_Day_Seconds'}
        agg_dict['PkgWatt'] = 'sum'
        df_daily = df.groupby(pd.Grouper(key='Time_Of_Day_Seconds', freq='D')).agg(agg_dict).reset_index()

        # figure
        plt.figure(figsize=(12, 6))
        sns.lineplot(x='Time_Of_Day_Seconds', y='PkgWatt', data=df_daily, marker='o')

        # title and labels
        plt.title(f'{base_filename} kWatt Time Series (Daily Aggregation)')
        plt.xlabel("Time")
        plt.ylabel("kWatts used")

        # x axis setting and ticks
        plt.xticks(rotation=45, ha='right')
        ax = plt.gca()
        ax.set_xticks(df_daily['Time_Of_Day_Seconds'])
        ax.set_xticklabels(df_daily['Time_Of_Day_Seconds'].dt.strftime('%Y-%m-%d'))

        # minor ticks
        plt.minorticks_on()

        # grid
        plt.grid(which='both', linestyle='--', linewidth=0.5)
        plt.grid(which='major', color='black', linestyle='-', linewidth=0.7)
        plt.grid(which='minor', color='gray', linestyle=':', linewidth=0.5)

        # setting the layout to tight
        plt.tight_layout()

        plt.savefig(f'{plots_folder}/{base_filename}_plot2.png')
        section1.add_content(f"{base_filename} Day Aggregated : Wattage","Wattage mapped over time",image_path=f'{plots_folder}/{base_filename}_plot2.png')
        # Show the plot
        # plt.show()

        start_time = df['Time_Of_Day_Seconds'].min()
        end_time = df['Time_Of_Day_Seconds'].max()
        num_hours = (end_time - start_time).total_seconds() / 3600

        # print(f"Start time: {start_time}")
        # print(f"End time: {end_time}")
        # print(f"Number of hours: {num_hours}")

        section1.add_content("Time Series information",f"Start datetime: {start_time}\n")
        section1.add_content("Time Series information",f"End datetime: {end_time}\n")
        section1.add_content("Time Series information",f"Number of hours: {num_hours}\n")

        df_energy_parameters = df.copy(deep=True)

        def average_power_consumption(df):
            return df['PkgWatt'].mean()

        def tdp_utilization(df, tdp):
            return (df['PkgWatt'] / tdp) * 100

        def idle_power_consumption(df):
            # Assuming idle periods are when Busy% is very low (e.g., below 15%)
            idle_periods = df[df['Busy%'] < 25]
            return idle_periods['PkgWatt'].mean()

        def dynamic_power_consumption(df):
            idle_power = idle_power_consumption(df)
            return df['PkgWatt'] - idle_power

        # Calculating average power consumption
        Avg_power_Server = average_power_consumption(df)

        # Thermal Utilisation Value for CPU assuming its Intel Xeon is 130W in the given Lenovo Server
        tdp_value = 130
        df_energy_parameters['tdp_util'] = tdp_utilization(df_energy_parameters, tdp_value)

        df_energy_parameters['idle_power'] = idle_power_consumption(df_energy_parameters)

        df_energy_parameters['dynamic_power'] = dynamic_power_consumption(df_energy_parameters)

        df_energy_parameters_head=df_energy_parameters.iloc[:,[0,1,2,3,4]].head()
        section1.add_content("Energy Parameters",df_energy_parameters_head)

        df_energy_parameters_head=df_energy_parameters.iloc[:,[0,5,6,7]].head()
        section1.add_content("Energy Parameters",df_energy_parameters_head)

        df_energy_parameters_head=df_energy_parameters.iloc[:,[0,5,6,7]].head()
        section1.add_content("Energy Parameters",df_energy_parameters_head)

        df__energy_hourly = df_energy_parameters.groupby(pd.Grouper(key='Time_Of_Day_Seconds', freq='H')).agg({
            'PkgWatt': 'mean',         # Mean PkgWatt
            'tdp_util': 'mean',        # Mean tdp_util
            'idle_power': 'mean',      # Mean idle_power
            'dynamic_power': 'mean'    # Mean dynamic_power
        }).reset_index()


        plt.figure(figsize=(12, 6))

        sns.lineplot(x='Time_Of_Day_Seconds', y='PkgWatt', data=df__energy_hourly, label='PkgWatt')
        sns.lineplot(x='Time_Of_Day_Seconds', y='tdp_util', data=df__energy_hourly, label='TDP Utilization (%)')
        sns.lineplot(x='Time_Of_Day_Seconds', y='idle_power', data=df__energy_hourly, label='Idle Power')
        sns.lineplot(x='Time_Of_Day_Seconds', y='dynamic_power', data=df__energy_hourly, label='Dynamic Power')

        plt.xticks(rotation=45, ha='right')

        plt.title(f'{base_filename} Hourly Energy Parameters')
        plt.xlabel("Time")
        plt.ylabel("Value")
        plt.tight_layout()
        plt.legend()
        plt.grid(True)

        plt.savefig(f'{plots_folder}/{base_filename}_plot3.png')
        section1.add_content(f"{base_filename} Hour Energy Parameters","Viewing Energy Parameters such as Idle and Dynamic Power",image_path=f'{plots_folder}/{base_filename}_plot3.png')
        # Show the plot
        # plt.show()

        def calculate_carbon_emissions(df, countries, conversion_factors):
            for country, conversion_factor in zip(countries, conversion_factors):
                # Convert conversion factor from gCO2/kWh to KgCO2
                # kWh => kW=1000 watts,hour=60 mins, 1 min =60 secs, kwh=3600000 watts-sec
                # 1 watt-second= 1 Joule
                gCO2_per_watt_second = (conversion_factor )/1000

                # Calculate carbon emissions per second
                column_name = f'kgCO2_per_watt_second_{country}'
                df[column_name] = gCO2_per_watt_second *(df['PkgWatt'])

            return df

        # IEA conversion factors
        countries = ['Norway', 'Iceland', 'Switzerland', 'Sweden', 'France', 'Latvia', 'Austria']
        conversion_factors = [1.3, 0.4, 6.1, 8.1, 15.8, 17.5, 19.6]

        # carbon emissions
        df = calculate_carbon_emissions(df, countries, conversion_factors)

        section1.add_content("Carbon Calculations inserted",df.iloc[:,[0,1,2,3]].head())
        section1.add_content("Carbon Calculations inserted",df.iloc[:,[0,4,5,6]].head())
        section1.add_content("Carbon Calculations inserted",df.iloc[:,[0,7,8]].head())
        section1.add_content("Carbon Calculations inserted",df.iloc[:,[0,9,10]].head())
        section1.add_content("Carbon Calculations inserted",df.iloc[:,[0,11,12]].head())
        section1.add_content("Carbon Calculations inserted",df.iloc[:,[0,13]].head())

        #Check2-Okay
        country_cols = [f'kgCO2_per_watt_second_{country}' for country in countries]
        country_sums=[
            df['kgCO2_per_watt_second_Norway'].sum(),
            df['kgCO2_per_watt_second_Iceland'].sum(),
            df['kgCO2_per_watt_second_Switzerland'].sum(),
            df['kgCO2_per_watt_second_Sweden'].sum(),
            df['kgCO2_per_watt_second_France'].sum(),
            df['kgCO2_per_watt_second_Latvia'].sum(),
            df['kgCO2_per_watt_second_Austria'].sum()]

        # Print the sum of PkgWatt and gCO2 for each country-Check3
        # print(f"Carbon emissions country wise for {end_time-start_time} :")
        pkg_watt_sum = df['PkgWatt'].sum()
        # print( f"Total wattage {pkg_watt_sum} KWatts")
        section1.add_content("Total Wattage of Server",f"Total wattage {pkg_watt_sum} KWatts")
        country_names = ['Norway', 'Iceland', 'Switzerland', 'Sweden', 'France', 'Latvia', 'Austria']
        # print(f'Estimated emissions if server {base_filename}  in alternate country \n')
        section1.add_content("Estimated emissions if server in alternate country","")
        for i, country_sum in enumerate(country_sums):
            # print(f"Estimated emissions {country_names[i]} for : {country_sum} kg")
            section1.add_content(f"Estimated emissions for {country_names[i]}",f"{country_sum} kg")



        df_min = df.groupby(pd.Grouper(key='Time_Of_Day_Seconds', freq='min')).agg(
            {
                'CPU': 'mean',
                'Core': 'mean',
                'Avg_MHz': 'mean',
                'Busy%': 'mean',
                'Bzy_MHz': 'mean',
                'PkgWatt': 'sum',  # Sum PkgWatt
                'kgCO2_per_watt_second_Norway': 'sum',
                'kgCO2_per_watt_second_Iceland': 'sum',
                'kgCO2_per_watt_second_Switzerland': 'sum',
                'kgCO2_per_watt_second_Sweden': 'sum',
                'kgCO2_per_watt_second_France': 'sum',
                'kgCO2_per_watt_second_Latvia': 'sum',
                'kgCO2_per_watt_second_Austria': 'sum'
            }
        ).reset_index()
        df_min=df_min.rename(columns={
                'Time_Of_Day_Seconds': 'Time_Of_Day_min',
                'kgCO2_per_watt_second_Norway': 'kgCO2_per_watt_min_Norway',
                'kgCO2_per_watt_second_Iceland': 'kgCO2_per_watt_min_Iceland',
                'kgCO2_per_watt_second_Switzerland': 'kgCO2_per_watt_min_Switzerland',
                'kgCO2_per_watt_second_Sweden': 'kgCO2_per_watt_min_Sweden',
                'kgCO2_per_watt_second_France': 'kgCO2_per_watt_min_France',
                'kgCO2_per_watt_second_Latvia': 'kgCO2_per_watt_min_Latvia',
                'kgCO2_per_watt_second_Austria': 'kgCO2_per_watt_min_Austria'
        })

        section1.add_content("Minute aggregation after adding carbon content",df_min.iloc[:,[0,1,2,3]].head())
        section1.add_content("Minute aggregation after adding carbon content",df_min.iloc[:,[0,4,5,6]].head())
        section1.add_content("Minute aggregation after adding carbon content",df_min.iloc[:,[0,7,8]].head())
        section1.add_content("Minute aggregation after adding carbon content",df_min.iloc[:,[0,9,10]].head())
        section1.add_content("Minute aggregation after adding carbon content",df_min.iloc[:,[0,11,12]].head())
        section1.add_content("Minute aggregation after adding carbon content",df_min.iloc[:,[0,13]].head())

        # figure with 4 subplots
        fig, axes = plt.subplots(4, 1, figsize=(12, 12))

        # PkgWatt on the first subplot
        sns.lineplot(x="Time_Of_Day_min", y="PkgWatt", data=df_min, ax=axes[0])
        axes[0].set_ylabel("PkgWatt")

        # kgCO2_per_watt_min_Norway on the second subplot
        sns.lineplot(x="Time_Of_Day_min", y="kgCO2_per_watt_min_Norway", data=df_min, ax=axes[1])
        axes[1].set_ylabel("kgCO2_per_watt_min_Norway")

        # kgCO2_per_watt_min_Iceland on the third subplot
        sns.lineplot(x="Time_Of_Day_min", y="kgCO2_per_watt_min_Iceland", data=df_min, ax=axes[2])
        axes[2].set_ylabel("kgCO2_per_watt_min_Iceland")

        # kgCO2_per_watt_min_Switzerland on the fourth subplot
        sns.lineplot(x="Time_Of_Day_min", y="kgCO2_per_watt_min_Switzerland", data=df_min, ax=axes[3])
        axes[3].set_ylabel("kgCO2_per_watt_min_Switzerland")


        # Set the x-axis label and rotate tick labels for all subplots
        for ax in axes:
            ax.set_xlabel("Time")
            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)

        # Adjust the spacing between subplots
        plt.tight_layout()

        plt.savefig(f'{plots_folder}/{base_filename}_plot4.png')
        section1.add_content(f"{base_filename} Carbon Emissions per Watt-min","Carbon emissions for the server per watt of energy consumed each min",image_path=f'{plots_folder}/{base_filename}_plot4.png')
        # Show the plot
        # plt.show()

        # Create a figure with 4 subplots
        fig, axes = plt.subplots(4, 1, figsize=(12, 12))

        # Plot kgCO2_per_watt_min_Sweden on the fourth subplot
        sns.lineplot(x="Time_Of_Day_min", y="kgCO2_per_watt_min_Sweden", data=df_min, ax=axes[0])
        axes[0].set_ylabel("kgCO2_per_watt_min_Sweden")

        # Plot kgCO2_per_watt_min_France on the fourth subplot
        sns.lineplot(x="Time_Of_Day_min", y="kgCO2_per_watt_min_France", data=df_min, ax=axes[1])
        axes[1].set_ylabel("kgCO2_per_watt_min_France")

        # Plot kgCO2_per_watt_min_Latvia on the fourth subplot
        sns.lineplot(x="Time_Of_Day_min", y="kgCO2_per_watt_min_Latvia", data=df_min, ax=axes[2])
        axes[2].set_ylabel("kgCO2_per_watt_min_Latvia")

        # Plot kgCO2_per_watt_min_Austria on the fourth subplot
        sns.lineplot(x="Time_Of_Day_min", y="kgCO2_per_watt_min_Austria", data=df_min, ax=axes[3])
        axes[3].set_ylabel("kgCO2_per_watt_min_Austria")

        # Set the x-axis label and rotate tick labels for all subplots
        for ax in axes:
            ax.set_xlabel("Time")
            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)

        # Adjust the spacing between subplots
        plt.tight_layout()

        plt.savefig(f'{plots_folder}/{base_filename}_plot5.png')
        section1.add_content(f"{base_filename} Carbon Emissions per Watt-min","Carbon emissions for the server per watt of energy consumed each min",image_path=f'{plots_folder}/{base_filename}_plot5.png')
        # Show the plot
        # plt.show()

        df_hourly = df.groupby(pd.Grouper(key='Time_Of_Day_Seconds', freq='H')).agg({
                'CPU': 'mean',
                'Core': 'mean',
                'Avg_MHz': 'mean',
                'Busy%': 'mean',
                'Bzy_MHz': 'mean',
                'PkgWatt': 'sum',  # Sum PkgWatt
                'kgCO2_per_watt_second_Norway': 'sum',
                'kgCO2_per_watt_second_Iceland': 'sum',
                'kgCO2_per_watt_second_Switzerland': 'sum',
                'kgCO2_per_watt_second_Sweden': 'sum',
                'kgCO2_per_watt_second_France': 'sum',
                'kgCO2_per_watt_second_Latvia': 'sum',
                'kgCO2_per_watt_second_Austria': 'sum'
        }).reset_index()

        df_hourly=df_hourly.rename(columns={
                'Time_Of_Day_Seconds': 'Time_Of_Day_hour',
                'kgCO2_per_watt_second_Norway': 'kgCO2_per_watt_hour_Norway',
                'kgCO2_per_watt_second_Iceland': 'kgCO2_per_watt_hour_Iceland',
                'kgCO2_per_watt_second_Switzerland': 'kgCO2_per_watt_hour_Switzerland',
                'kgCO2_per_watt_second_Sweden': 'kgCO2_per_watt_hour_Sweden',
                'kgCO2_per_watt_second_France': 'kgCO2_per_watt_hour_France',
                'kgCO2_per_watt_second_Latvia': 'kgCO2_per_watt_hour_Latvia',
                'kgCO2_per_watt_second_Austria': 'kgCO2_per_watt_hour_Austria'
        })

        section1.add_content("Hour aggregation after adding carbon content",df_hourly.iloc[:,[0,1,2,3]].head())
        section1.add_content("Hour aggregation after adding carbon content",df_hourly.iloc[:,[0,4,5,6]].head())
        section1.add_content("Hour aggregation after adding carbon content",df_hourly.iloc[:,[0,7,8]].head())
        section1.add_content("Hour aggregation after adding carbon content",df_hourly.iloc[:,[0,9,10]].head())
        section1.add_content("Hour aggregation after adding carbon content",df_hourly.iloc[:,[0,11,12]].head())
        section1.add_content("Hour aggregation after adding carbon content",df_hourly.iloc[:,[0,13]].head())

        fig, axes = plt.subplots(2, 1, figsize=(12, 10))

        # Plot PkgWatt on the first subplot
        sns.lineplot(x='Time_Of_Day_hour', y='PkgWatt', data=df_hourly, ax=axes[0])

        axes[0].set_title('PkgWatt over Time')
        axes[0].set_xlabel(" Time in hours ")
        axes[0].set_ylabel(" Wattage")
        # Plot kgCO2 emissions for each country on the second subplot
        for country in countries:
            column_name = f'kgCO2_per_watt_hour_{country}'
            sns.lineplot(x='Time_Of_Day_hour', y=column_name, data=df_hourly, ax=axes[1], label=country)

        axes[1].set_title('kgCO2 Emissions per hour for Different Countries')
        axes[1].set_xlabel(" Time in hours ")
        axes[1].set_ylabel(" KG of CO2eq")
        axes[1].legend(loc='lower right')

        for ax in axes:
            ax.set_xlabel("Time in hours")
            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)

        # Adjust spacing between subplots
        plt.tight_layout()

        plt.savefig(f'{plots_folder}/{base_filename}_plot6.png')
        section1.add_content(f"{base_filename} Carbon Emissions per Watt-hour","Carbon emissions for the server per watt of energy consumed each hour for all countries",image_path=f'{plots_folder}/{base_filename}_plot6.png')
        # Show the plot
        # plt.show()

        df_daily = df.groupby(pd.Grouper(key='Time_Of_Day_Seconds', freq='D')).agg({
                'CPU': 'mean',
                'Core': 'mean',
                'Avg_MHz': 'mean',
                'Busy%': 'mean',
                'Bzy_MHz': 'mean',
                'PkgWatt': 'sum',  # Sum PkgWatt
                'kgCO2_per_watt_second_Norway': 'sum',
                'kgCO2_per_watt_second_Iceland': 'sum',
                'kgCO2_per_watt_second_Switzerland': 'sum',
                'kgCO2_per_watt_second_Sweden': 'sum',
                'kgCO2_per_watt_second_France': 'sum',
                'kgCO2_per_watt_second_Latvia': 'sum',
                'kgCO2_per_watt_second_Austria': 'sum'
        }).reset_index()
        df_daily=df_daily.rename(columns={
                'Time_Of_Day_Seconds': 'Time_Of_Day_daily',
                'kgCO2_per_watt_second_Norway': 'kgCO2_per_watt_daily_Norway',
                'kgCO2_per_watt_second_Iceland': 'kgCO2_per_watt_daily_Iceland',
                'kgCO2_per_watt_second_Switzerland': 'kgCO2_per_watt_daily_Switzerland',
                'kgCO2_per_watt_second_Sweden': 'kgCO2_per_watt_daily_Sweden',
                'kgCO2_per_watt_second_France': 'kgCO2_per_watt_daily_France',
                'kgCO2_per_watt_second_Latvia': 'kgCO2_per_watt_daily_Latvia',
                'kgCO2_per_watt_second_Austria': 'kgCO2_per_watt_daily_Austria'
        })

        section1.add_content("Daily aggregation after adding carbon content",df_daily.iloc[:,[0,1,2,3]].head())
        section1.add_content("Daily aggregation after adding carbon content",df_daily.iloc[:,[0,4,5,6]].head())
        section1.add_content("Daily aggregation after adding carbon content",df_daily.iloc[:,[0,7,8]].head())
        section1.add_content("Daily aggregation after adding carbon content",df_daily.iloc[:,[0,9,10]].head())
        section1.add_content("Daily aggregation after adding carbon content",df_daily.iloc[:,[0,11,12]].head())
        section1.add_content("Daily aggregation after adding carbon content",df_daily.iloc[:,[0,13]].head())

        # subplots for PkgWatt and kgCO2 emissions
        fig, axes = plt.subplots(2, 1, figsize=(12, 10))

        # PkgWatt on the first subplot
        sns.lineplot(x='Time_Of_Day_daily', y='PkgWatt', data=df_daily, ax=axes[0])
        axes[0].set_title('PkgWatt over Time')
        axes[0].set_xlabel(" Time in days ")
        axes[0].set_ylabel(" Wattage")

        # formating the x-axis for the first subplot
        axes[0].set_xticks(df_daily['Time_Of_Day_daily'])
        axes[0].set_xticklabels(df_daily['Time_Of_Day_daily'].dt.strftime('%Y-%m-%d'))
        axes[0].minorticks_on()
        axes[0].grid(which='both', linestyle='--', linewidth=0.5)
        axes[0].grid(which='major', color='black', linestyle='-', linewidth=0.7)
        axes[0].grid(which='minor', color='gray', linestyle=':', linewidth=0.5)

        # Ploting kgCO2 emissions for each country on the second subplot
        for country in countries:
            column_name = f'kgCO2_per_watt_daily_{country}'
            sns.lineplot(x='Time_Of_Day_daily', y=column_name, data=df_daily, ax=axes[1], label=country)

        axes[1].set_title('kgCO2 Emissions per watt-daily for Different Countries')
        axes[1].set_xlabel(" Time in days ")
        axes[1].set_ylabel(" Grams of CO2eq")
        axes[1].legend(loc='lower right')

        # Formatin x-axis for the second subplot
        axes[1].set_xticks(df_daily['Time_Of_Day_daily'])
        axes[1].set_xticklabels(df_daily['Time_Of_Day_daily'].dt.strftime('%Y-%m-%d'))
        axes[1].minorticks_on()
        axes[1].grid(which='both', linestyle='--', linewidth=0.5)
        axes[1].grid(which='major', color='black', linestyle='-', linewidth=0.7)
        axes[1].grid(which='minor', color='gray', linestyle=':', linewidth=0.5)

        for ax in axes:
            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)

        plt.tight_layout()

        plt.savefig(f'{plots_folder}/{base_filename}_plot7.png')
        section1.add_content(f"{base_filename} Carbon Emissions per Watt-day","Carbon emissions for the server per watt of energy consumed each day",image_path=f'{plots_folder}/{base_filename}_plot7.png')
        # plt.show()

        # Check3-Okay
        def transform_dataframe(df,time_set, location_name1):
            # Converting location_name1 to uppercase
            location_name1 = location_name1.upper()

            # Mapping location_name1 to the respective country so we get server
            # location for selection
            location_map = {
                'GLCHBS': 'Switzerland',
                'GLCHST': 'Austria'
            }

            location = location_map.get(location_name1)

            if location is None:
                raise ValueError(f"Invalid location name: {location_name1}")

            carbon_column = f'kgCO2_per_watt_{time_set}_{location}'

            if carbon_column not in df.columns:
                raise ValueError(f"Column '{carbon_column}' not found in DataFrame")

            df['Busy_Period_freq'] = df.apply(lambda row: row['Bzy_MHz'] if row['Busy%'] > 25 else row['Avg_MHz'], axis=1)
            # Logic: if the server is not busy but average frequency is high meaning it is using more power despite being on low load
            new_df = df[[
                f'Time_Of_Day_{time_set}','CPU', 'Core', 'Avg_MHz', 'Busy%', 'Bzy_MHz','Busy_Period_freq',
                carbon_column
            ]]
            return new_df

        # Creating dataframe for modelling
        df_transformed_hour = transform_dataframe(df_hourly,time_set="hour", location_name1=location_name)
        df_transformed_min=transform_dataframe(df_min,time_set="min", location_name1=location_name)# To cross check all tests if increasing data points helps

        section1.add_content("Data transformation for initial analysis","Removed unnecessary columns and based on server name selected carbon emission of that region")

        """## Initial Analysis"""

        def initial_analysis(data,section,timeperiod_setting,plots_folder,output_folder,base_filename):

            # Basic plot-Check1
            data.plot(subplots=True, figsize=(12, 18), layout=(8, 1), linestyle='-')
            plt.suptitle('Time Series Plots')
            plt.xlabel('Date')
            plt.savefig(f'{plots_folder}/{base_filename}_plot8.png')
            # print("Plotted at {plots_folder}/{base_filename}_plot8.png")
            section.add_content("Time Series Plots for all features","Time_Series plots",image_path=f'{plots_folder}/{base_filename}_plot8.png')
            # plt.show()

            # Tests for Stationarity
            def adf_test(timeseries):
                result = adfuller(timeseries)
                # print(f'ADF Statistic: {result[0]}')
                # print(f'p-value: {result[1]}')
                # print('Critical Values:')
                # for key, value in result[4].items():
                #     print(f'{key}: {value}')
                return result

            for column in data.columns:
                result=adf_test(data[column])
                section.add_content(f"Stationarity ADF Test for {column}",f"\n ADF Statistic: {result[0]} \n  P-Value: {result[1]}")
                for key, value in result[4].items():
                    section.add_content(f' ADF Critical Values {column}',f'\n {key}, {value}')

            # Kwiatkowski-Phillips-Schmidt-Shin Test (KPSS Test)
            def kpss_test(timeseries,column):
                result = kpss(timeseries, regression='c', nlags="auto")
                # print(f"KPSS Test for {column}")
                # print(f"KPSS Statistic: {result[0]}")
                # print(f"P-Value: {result[1]}")
                # print("Critical Values:")
                # for key, value in result[3].items():
                #     print(f"{key}: {value}")
                return result
            for column in data.columns:
                result=kpss_test(data[column],column)
                section.add_content(f"Stationarity KPSS Test for {column}",f"\n KPSS Statistic: {result[0]} \n  P-Value: {result[1]}")

            # ACF and PACF
            for i,column in enumerate(data.columns):
                plt.figure(figsize=(12, 6))

                plt.subplot(121)
                plot_acf(data[column], ax=plt.gca(), lags=10)
                plt.title(f'Autocorrelation Function for {column}')

                plt.subplot(122)
                plot_pacf(data[column], ax=plt.gca(), lags=10)
                plt.title(f'Partial Autocorrelation Function for {column}')
                plt.savefig(f'{plots_folder}/{base_filename}_plot{i+9}.png')
                section.add_content("PACF and ACF","PACF(opposite of ACF) and ACF(need low p value to disprove null hypothesis which is non stationary)",image_path=f'{plots_folder}/{base_filename}_plot{i+9}.png')
                # plt.show()

            # Correlation heatmap-Check1end
            corr_matrix = data.corr()

            plt.figure(figsize=(10, 8))
            sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', square=True)
            plt.title('Correlation Matrix Heatmap')
            plt.savefig(f'{plots_folder}/{base_filename}_plot14.png')
            section.add_content("Correlation","Correlation Heatmap",image_path=f'{plots_folder}/{base_filename}_plot14.png')
            # plt.show()

            # Rolling Statistics
            window = 5  # Small window due to short series

            for i,column in enumerate(data.columns):
                rolling_mean = data[column].rolling(window=window).mean()
                rolling_std = data[column].rolling(window=window).std()

                plt.figure(figsize=(12, 6))
                plt.plot(data[column], label='Original', marker='o')
                plt.plot(rolling_mean, label='Rolling Mean', marker='o')
                plt.plot(rolling_std, label='Rolling Std', marker='o')
                plt.title(f'Rolling Mean & Standard Deviation for {column}')
                plt.xlabel('Date')
                plt.ylabel('Value')
                plt.legend()
                plt.grid(True)
                plt.savefig(f'{plots_folder}/{base_filename}_plot{i+15}.png')
                section.add_content("Rollig Stats","Rolling mean and std dev",image_path=f'{plots_folder}/{base_filename}_plot{i+15}.png')
                # plt.show()

            # Plot for each feature
            for i,column in enumerate(data.columns):
                plt.figure(figsize=(12, 6))
                sns.histplot(data[column], kde=True)
                plt.title(f'Histogram and Density Plot for {column}')
                plt.xlabel('Value')
                plt.ylabel('Frequency')
                plt.grid(True)
                plt.savefig(f'{plots_folder}/{base_filename}_plot{i+21}.png')
                section.add_content("Hist Plots","Hist plots",image_path=f'{plots_folder}/{base_filename}_plot{i+21}.png')
                # plt.show()

            # LAg Plot
            for i,column in enumerate(data.columns):
                plt.figure(figsize=(6, 6))
                lag_plot(data[column])
                plt.title(f'Lag Plot for {column}')
                plt.xlabel('Value at t')
                plt.ylabel('Value at t+1')
                plt.grid(True)
                plt.savefig(f'{plots_folder}/{base_filename}_plot{i+27}.png')
                section.add_content("Lag Plots","Lag Plots",image_path=f'{plots_folder}/{base_filename}_plot{i+27}.png')
                # plt.show()

            # Scatter plot-Check3 Ookay
            pd.plotting.scatter_matrix(data, figsize=(12, 12), diagonal='kde')
            plt.suptitle('Scatter Plot Matrix')
            plt.grid(True)
            plt.savefig(f'{plots_folder}/{base_filename}_plot33.png')
            section.add_content("Scatter Plot","Scatter matrix",image_path=f'{plots_folder}/{base_filename}_plot33.png')
            # plt.show()
            return section

        """# Untreated Data"""

        timeperiod_setting="hour"

        section2 = PDFSection("Initial Analysis including tests for stationarity, autocorrelation etc with untreated data")

        if timeperiod_setting=="hour":
            selected_df = df_transformed_hour
        elif timeperiod_setting=="min":
            selected_df = df_transformed_min
        else:
            selected_df = df_transformed_hour

        data=selected_df.copy(deep=True)

        data.set_index(f'Time_Of_Day_{timeperiod_setting}', inplace=True)

        data.drop(['Bzy_MHz','Avg_MHz'], axis=1, inplace=True) # Already derived feature available
        data = data.iloc[1:-1] # Removing first and last row due to outliers

        # Features and target variable
        y = data[f'kgCO2_per_watt_{timeperiod_setting}_{location_country}']
        X = data.drop(columns=f'kgCO2_per_watt_{timeperiod_setting}_{location_country}')

        section2=initial_analysis(data,section2,timeperiod_setting,plots_folder1,output_folder1,base_filename)

        """# Lag based data"""

        section3 = PDFSection("Initial Analysis including tests for stationarity, autocorrelation etc with treated data")

        data_lag=data.copy(deep=True)

        data_lag['Busy_lag1'] = data_lag['Busy%'].shift(1)
        data_lag['Busy_lag2'] = data_lag['Busy%'].shift(2)
        data_lag[f'kgCO2_per_watt_{timeperiod_setting}_{location_country}_lag1'] = data_lag[f'kgCO2_per_watt_{timeperiod_setting}_{location_country}'].shift(1)
        data_lag[f'kgCO2_per_watt_{timeperiod_setting}_{location_country}_lag1'] = data_lag[f'kgCO2_per_watt_{timeperiod_setting}_{location_country}'].shift(1)

        data_lag.dropna(inplace=True)  # Remove rows with NaN values due to lagging

        # Features and target variable
        y_lag = data_lag[f'kgCO2_per_watt_{timeperiod_setting}_{location_country}']
        X_lag = data_lag.drop(columns=f'kgCO2_per_watt_{timeperiod_setting}_{location_country}')

        section3.add_content("Lagged values for Busy and kgCO2_per_watt since they showed linear behaviour",data_lag.head())

        """# PCA, PCA-lag"""

        data_pca=data.copy(deep=True)

        # Assuming df is your DataFrame and 'kgCO2_per_watt_hour_Switzerland' is the target feature
        def pca_decomposing(data_pca,timeperiod_setting,location_country):
          features = data_pca.drop(columns=[f'kgCO2_per_watt_{timeperiod_setting}_{location_country}'])
          target = data_pca[f'kgCO2_per_watt_{timeperiod_setting}_{location_country}']

          # Standardize the features
          scaler = StandardScaler()
          features_scaled = scaler.fit_transform(features)

          # Apply PCA
          pca = PCA(n_components=0.95)  # Retain 95% of variance
          pca_components = pca.fit_transform(features_scaled)

          # Create a DataFrame for the PCA components
          pca_df = pd.DataFrame(pca_components, columns=[f'PC{i+1}' for i in range(pca_components.shape[1])])
          pca_df.index = data_pca.index
          return pca_df,target

        X_pca,y_pca=pca_decomposing(data_pca,timeperiod_setting,location_country)
        data_pca_2=pd.concat([X_pca,y_pca],axis=1)

        section3.add_content("PCA values",data_pca_2.head())

        # Train-test split
        X_pca_lag,y_pca_lag=pca_decomposing(data_lag,timeperiod_setting,location_country)
        data_pca_lag=pd.concat([X_pca_lag,y_pca_lag],axis=1)

        section3.add_content("PCA_Lagged values",data_pca_lag.head())

        section4 = PDFSection("Initial Analysis Lagged Data")

        section4=initial_analysis(data_lag,section4,timeperiod_setting,plots_folder2,output_folder2,base_filename)

        section5 = PDFSection("Initial Analysis PCA Data")

        section5=initial_analysis(data_pca_2,section5,timeperiod_setting,plots_folder3,output_folder3,base_filename)



        """## Models"""

        def model_analysis(X,y,section,timeperiod_setting,plots_folder,output_folder,base_filename,location_country):
            # MinMaxScaler
            scaler = MinMaxScaler()

            # apply to the DataFrame
            X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)
            X_normalized.index=X.index

            # print(X_normalized.head())

            # StandardScaler
            scaler = StandardScaler()

            # apply to the DataFrame
            X_standardized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)
            X_standardized.index=X.index
            # print(X_standardized.head())

            # RobustScaler - trying to offeset outliers using median and interquartile range
            scaler = RobustScaler()

            # apply to the DataFrame
            X_robust = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)
            X_robust.index=X.index
            # print(X_robust.head())

            section.add_content(f"Spitting the data into Features and Target component {f'kgCO2_per_watt_{timeperiod_setting}_{location_country}'}"," We additionally dropped Avg_MHz and Busy_MHz, as Busy_period_freq already accounts for both of them")
            section.add_content(f"Features:",X.head())
            section.add_content(f"Target:",y.head())
            section.add_content(f"Normalized Features:",X_normalized.head())
            section.add_content(f"Standardized Features:",X_standardized.head())
            section.add_content(f"Robust Features:",X_robust.head())

            X1 = sm.add_constant(X)  # Adding a constant term to the feature to run OLS models
            X1_normalized = sm.add_constant(X_normalized)
            X1_standardized = sm.add_constant(X_standardized)
            X1_robust = sm.add_constant(X_robust)

            section.add_content(f"Added constant to check if scaling has an affect on the models","Non-Scaled,Normalised,Standardised,Robust-Scaling")
            section.add_content(f"Features Unscaled:",X1.head())
            section.add_content(f"Features Normalized:",X1_normalized.head())
            section.add_content(f"Features Standardized:",X1_standardized.head())
            section.add_content(f"Features Robust:",X1_robust.head())

            # Features without scaling
            # Checking residuals for OLS Model1 to check for heteroscedasticity

            model1 = sm.OLS(y, X1).fit()
            residuals = model1.resid

            # Plot residuals
            plt.figure(figsize=(12, 8))
            plt.plot(residuals, marker='o', linestyle='-')
            plt.title('Residuals Plot')
            plt.xlabel('Date')
            plt.ylabel('Residuals')
            plt.grid(True)

            plt.savefig(f'{plots_folder}/{base_filename}_plot34.png')
            section.add_content("OLS Model Summary", model1.summary(), is_model_summary=True)
            section.add_content("OLS Model with non-scaled features","Odinary Least Square method without feature scaling",image_path=f'{plots_folder}/{base_filename}_plot34.png')
            plt.show()
            # print(model1.summary())

            # Plot residuals against fitted values
            plt.figure(figsize=(12, 6))
            plt.scatter(model1.fittedvalues, residuals)
            plt.title('Residuals vs Fitted Values')
            plt.xlabel('Fitted Values')
            plt.ylabel('Residuals')
            plt.grid(True)
            plt.savefig(f'{plots_folder}/{base_filename}_plot35.png')
            section.add_content("Scatter Plot","Scatter matrix",image_path=f'{plots_folder}/{base_filename}_plot35.png')
            plt.show()

            # X with normalised scaling
            # Checking residuals for OLS Model2 to check for heteroscedasticity

            model2 = sm.OLS(y, X1_normalized).fit()
            residuals = model2.resid

            # Plot residuals
            plt.figure(figsize=(12, 8))
            plt.plot(residuals, marker='o', linestyle='-')
            plt.title('Residuals Plot')
            plt.xlabel('Date')
            plt.ylabel('Residuals')
            plt.grid(True)

            plt.savefig(f'{plots_folder}/{base_filename}_plot36.png')
            section.add_content("OLS Model Summary: with normal scaling", model2.summary(), is_model_summary=True)
            section.add_content("OLS Model with non-scaled features","Odinary Least Square method with normal scaling",image_path=f'{plots_folder}/{base_filename}_plot36.png')
            plt.show()
            # print(model2.summary())

            # Plot residuals against fitted values
            plt.figure(figsize=(12, 6))
            plt.scatter(model2.fittedvalues, residuals)
            plt.title('Residuals vs Fitted Values')
            plt.xlabel('Fitted Values')
            plt.ylabel('Residuals')
            plt.grid(True)

            plt.savefig(f'{plots_folder}/{base_filename}_plot37.png')
            section.add_content("Scatter Plot","Scatter matrix",image_path=f'{plots_folder}/{base_filename}_plot37.png')
            plt.show()

            # X with standard scaling
            # Checking residuals for OLS Model3 to check for heteroscedasticity

            model3 = sm.OLS(y, X1_standardized).fit()
            residuals = model3.resid

            # Plot residuals
            plt.figure(figsize=(12, 8))
            plt.plot(residuals, marker='o', linestyle='-')
            plt.title('Residuals Plot')
            plt.xlabel('Date')
            plt.ylabel('Residuals')
            plt.grid(True)

            plt.savefig(f'{plots_folder}/{base_filename}_plot38.png')
            section.add_content("OLS Model Summary: with standard scaling", model3.summary(), is_model_summary=True)
            section.add_content("OLS Model with non-scaled features","Odinary Least Square method with standard scaling",image_path=f'{plots_folder}/{base_filename}_plot38.png')
            plt.show()
            print(model3.summary())

            # Plot residuals against fitted values
            plt.figure(figsize=(12, 6))
            plt.scatter(model3.fittedvalues, residuals)
            plt.title('Residuals vs Fitted Values')
            plt.xlabel('Fitted Values')
            plt.ylabel('Residuals')
            plt.grid(True)

            plt.savefig(f'{plots_folder}/{base_filename}_plot39.png')
            section.add_content("Scatter Plot","Scatter matrix",image_path=f'{plots_folder}/{base_filename}_plot39.png')
            plt.show()

            # X with robust scaling allowing to manage outliers
            # Checking residuals for OLS Model4 to check for heteroscedasticity

            model4 = sm.OLS(y, X1_robust).fit()
            residuals = model4.resid

            # Plot residuals
            plt.figure(figsize=(12, 8))
            plt.plot(residuals, marker='o', linestyle='-')
            plt.title('Residuals Plot')
            plt.xlabel('Date')
            plt.ylabel('Residuals')
            plt.grid(True)
            plt.savefig(f'{plots_folder}/{base_filename}_plot40.png')
            section.add_content("OLS Model Summary: with robust scaling", model4.summary(), is_model_summary=True)
            section.add_content("OLS Model with non-scaled features","Odinary Least Square method with robust(median+interquartile) scaling",image_path=f'{plots_folder}/{base_filename}_plot40.png')
            plt.show()
            # print(model4.summary())

            # Plot residuals against fitted values
            plt.figure(figsize=(12, 6))
            plt.scatter(model4.fittedvalues, residuals)
            plt.title('Residuals vs Fitted Values')
            plt.xlabel('Fitted Values')
            plt.ylabel('Residuals')
            plt.grid(True)
            plt.savefig(f'{plots_folder}/{base_filename}_plot41.png')
            section.add_content("Scatter Plot","Scatter matrix",image_path=f'{plots_folder}/{base_filename}_plot41.png')
            plt.show()

            # Checking for autocorrelations
            vif_data = pd.DataFrame()
            vif_data["Feature"] = X.columns
            vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

            section.add_content("OLS without scaling",vif_data)

            # The Ljung-Box test to check for autocorrelation in the residuals.

            lb_test = acorr_ljungbox(residuals, lags=[10], return_df=True)
            section.add_content("Lijung-Box Test",pd.DataFrame(lb_test))

            # Summary of the Stats
            section.add_content("Residual values",pd.DataFrame(residuals.describe()))

            #plottin
            plt.figure(figsize=(12, 6))
            sns.histplot(residuals, kde=True)
            plt.title('Histogram and Density Plot of Residuals')
            plt.xlabel('Residuals')
            plt.ylabel('Frequency')
            plt.grid(True)
            plt.savefig(f'{plots_folder}/{base_filename}_plot42.png')
            section.add_content("Hist Plots and Density Plots of Residuals from OLS without scaling","Odinary Least Square method without scaling",image_path=f'{plots_folder}/{base_filename}_plot42.png')
            plt.show()

            plt.figure(figsize=(12, 6))

            plt.subplot(121)
            plot_acf(residuals, ax=plt.gca(), lags=10)
            plt.title('Autocorrelation Function of Residuals')

            plt.subplot(122)
            plot_pacf(residuals, ax=plt.gca(), lags=10)
            plt.title('Partial Autocorrelation Function of Residuals')
            plt.savefig(f'{plots_folder}/{base_filename}_plot43.png')
            section.add_content("ACF and PACF of Residuals from OLS without scaling","Odinary Least Square method without scaling",image_path=f'{plots_folder}/{base_filename}_plot43.png')
            plt.show()

            # Decomposing each feature's time series-similiar to what we did in R
            decomposed = {}
            for col in data2.columns:
                result = seasonal_decompose(data2[col], model='additive', period=24)  # Assuming daily seasonality (24 hours) since we only have few data points
                decomposed[col] = result

            # Plot decomposed components
            plt.figure(figsize=(12, 10))
            for i, (col, result) in enumerate(decomposed.items()):
                plt.subplot(len(decomposed), 1, i + 1)
                plt.plot(result.trend, label='Trend')
                plt.plot(result.seasonal, label='Seasonal')
                plt.plot(result.resid, label='Residual')
                plt.title(f'Decomposition of {col}')
                plt.legend()

            plt.tight_layout()
            plt.savefig(f'{plots_folder}/{base_filename}_plot44.png')
            section.add_content("Features Decomposition","Looking at level,trend and seasonality",image_path=f'{plots_folder}/{base_filename}_plot44.png')
            plt.show()

            # Linear Regression -Plots at fig 51 onwards
            section.add_content("Regression-Linear,Ridge",f"Plots are saved 52 number onwards")
            X_train_regression, X_test_regression, y_train_regression, y_test_regression = train_test_split(X, y, test_size=0.2, random_state=42)
            lr_model = LinearRegression()
            lr_model.fit(X_train_regression, y_train_regression)
            y_pred = lr_model.predict(X_test_regression)
            y_test_sorted = y_test_regression.sort_index()
            y_pred_linear_sorted = pd.Series(y_pred, index=y_test_regression.index).sort_index()

            # Error Metrics
            mse = mean_squared_error(y_test_regression, y_pred)
            mae = mean_absolute_error(y_test_regression, y_pred)
            rmse = np.sqrt(mse)

            # print(f'MSE: {mse}')
            # print(f'MAE: {mae}')
            # print(f'RMSE: {rmse}')

            lr_summary = pd.DataFrame({
                'Feature': X_train_regression.columns,
                'Coefficient': lr_model.coef_
            })

            section.add_content("Linear Regression",f"MSE:{mse}")
            section.add_content("Linear Regression",f"MAE:{mae}")
            section.add_content("Linear Regression",f"RMSE:{rmse}")
            section.add_content("Linear Regression Predicted",pd.DataFrame(y_pred_linear_sorted))
            section.add_content("Linear Regression",f"Coefficients summary: {lr_summary}")
            section.add_content("Linear Regression",f"Predicted output saved: {output_folder}/{base_filename}-LinearRegression_Predicted_Values.xlsx")

            # print(lr_summary)

            # Plotting the predictions vs actual values
            plt.figure(figsize=(10, 5))
            plt.plot(y_test_sorted.index, y_test_sorted, label='Actual')
            plt.plot(y_test_sorted.index, y_pred_linear_sorted, label='Predicted')
            plt.xlabel('Time')
            plt.ylabel('kgCO2_per_watt_hour_Switzerland')
            plt.title('Actual vs Predicted Values')
            plt.legend()

            plt.savefig(f'{plots_folder}/{base_filename}_plot52.png')
            section.add_content("Linear Regression Model","Basic Linear regression model",image_path=f'{plots_folder}/{base_filename}_plot52.png')
            # plt.show()

            pd.DataFrame(pd.DataFrame(y_pred_linear_sorted), columns=[f'{base_filename} Linear Regression Predicted Values']).to_excel(f'{output_folder}/{base_filename}-LinearRegression_Predicted_Values.xlsx', index=True)
            print(y_pred_linear_sorted)


            # Ridge Regression
            ridge = Ridge(alpha=1.0)
            ridge.fit(X_train_regression, y_train_regression)
            y_pred_ridge = ridge.predict(X_test_regression)
            y_test_sorted = y_test_regression.sort_index()
            y_pred_ridge_sorted = pd.Series(y_pred_ridge, index=y_test_regression.index).sort_index()

            # Error Metrics for Ridge Regression
            ridge_mse = mean_squared_error(y_test_regression, y_pred_ridge)
            ridge_mae = mean_absolute_error(y_test_regression, y_pred_ridge)
            ridge_rmse = np.sqrt(ridge_mse)

            # print(f'Ridge Regression MSE: {ridge_mse}')
            # print(f'Ridge Regression MAE: {ridge_mae}')
            # print(f'Ridge Regression RMSE: {ridge_rmse}')

            ridge_summary = pd.DataFrame({
                'Feature': X_train_regression.columns,
                'Coefficient': ridge.coef_
            })

            section.add_content("Ridge Regression",f"MSE:{ridge_mse}")
            section.add_content("Ridge Regression",f"MAE:{ridge_mae}")
            section.add_content("Ridge Regression",f"RMSE:{ridge_rmse}")
            section.add_content("Ridge Regression Predicted",pd.DataFrame(y_pred_ridge_sorted))
            section.add_content("Ridge Regression",f"Coefficients summary: {ridge_summary}")
            section.add_content("Ridge Regression",f"Predicted output saved: {output_folder}/{base_filename}-RidgeRegression_Predicted_Values.xlsx")
            # print(ridge_summary)

            # Plotting Ridge Regression results
            plt.figure(figsize=(14, 6))
            plt.plot(y_test_sorted.index, y_test_sorted, label='Actual', marker='o')
            plt.plot(y_test_sorted.index, y_pred_ridge_sorted, label='Predicted', marker='x')
            plt.xlabel('Time')
            plt.ylabel('kgCO2_per_watt_hour_Switzerland')
            plt.title('Ridge Regression: Actual vs Predicted')
            plt.legend()

            plt.savefig(f'{plots_folder}/{base_filename}_plot53.png')
            section.add_content("Ridge Regression Model","Basic Ridge regression model",image_path=f'{plots_folder}/{base_filename}_plot53.png')
            # plt.show()

            pd.DataFrame(pd.DataFrame(y_pred_ridge_sorted), columns=[f'{base_filename} Ridge Regression Predicted Values']).to_excel(f'{output_folder}/{base_filename}-RidgeRegression_Predicted_Values.xlsx', index=True)
            # print(y_pred_linear_sorted)


            ## Simple Moving Average Method

            date_sma=data2.index

            # Calculate Simple Moving Average (SMA) for the target column
            if timeperiod_setting=="hour":
                window_size = 6  # Example window size of 16 hours (bi-daily SMA)
            elif timeperiod_setting=="min":
                window_size = 60  # Example window size of 60 min (hourly SMA)
            sma_target = data2[f'kgCO2_per_watt_{timeperiod_setting}_{location_country}'].rolling(window=window_size).mean().dropna()

            # Calculate historical standard deviation for confidence interval
            historical_std = data2[f'kgCO2_per_watt_{timeperiod_setting}_{location_country}'].rolling(window=window_size).std().dropna()

            # Forecast future values using SMA for the target column
            forecast_steps = 10  # Number of steps to forecast (e.g., next 10 time periods)
            last_sma_target_value = sma_target.iloc[-1]  # Get the last SMA value for the target column

            # Repeat the last SMA value for the forecast steps
            forecasted_target_values = pd.Series(np.repeat(last_sma_target_value, forecast_steps),
                                                index=pd.date_range(start=date_sma[-1] + pd.Timedelta(hours=1), periods=forecast_steps, freq='H'))

            # print(f'Carbon Emissions per {timeperiod_setting}: \n {forecasted_target_values}')

            # Plot original data, SMA, and forecasted values for the target column with confidence interval
            plt.figure(figsize=(12, 6))
            plt.plot(data2.index, data2[f'kgCO2_per_watt_{timeperiod_setting}_{location_country}'], label='Target Column')
            plt.plot(sma_target.index, sma_target, label=f'SMA ({window_size} hours)')
            plt.fill_between(sma_target.index, sma_target - 1.96 * historical_std, sma_target + 1.96 * historical_std, color='gray', alpha=0.2, label='95% Confidence Interval')
            plt.plot(forecasted_target_values.index, forecasted_target_values, linestyle='--', label='Forecasted Target Values')
            plt.title(f'Forecasting Target Column Using SMA with Confidence Interval')
            plt.xlabel('Date')
            plt.ylabel('Value')
            plt.legend()
            plt.grid(True)
            plt.savefig(f'{plots_folder}/{base_filename}_plot45.png')

            section.add_content("SMA","Simple Moving Average with 6 hour window",image_path=f'{plots_folder}/{base_filename}_plot45.png')
            section.add_content("SMA",f"Forecasted output: Carbon Emissions per {timeperiod_setting}: \n")
            section.add_content("SMA",pd.DataFrame(forecasted_target_values))
            section.add_content("SMA",f"Forecasted output saved: {output_folder}/{base_filename}-SMA_Predicted_Values.xlsx")

            plt.show()
            # Save the forecasted values
            pd.DataFrame(forecasted_target_values, columns=[f'{base_filename} SMA Predicted Values']).to_excel(f'{output_folder}/{base_filename}-SMA_Predicted_Values.xlsx', index=True)

            """## Simple Exponential Smoothing"""

            data_column = f'kgCO2_per_watt_{timeperiod_setting}_{location_country}'

            alpha = 0.2  # smoothing factor
            forecast_steps = 20  # forecasting for next 20 periods

            # ses for carbon emissions
            ses_model = SimpleExpSmoothing(data2[data_column])
            ses_fit = ses_model.fit(smoothing_level=alpha, optimized=False)
            ses_forecast = ses_fit.forecast(steps=forecast_steps)

            # actual values
            actual_values = data2[data_column].iloc[-forecast_steps:]

            # calculating error
            rmse = np.sqrt(mean_squared_error(actual_values, ses_forecast))
            mape = np.mean(np.abs((actual_values - ses_forecast) / actual_values)) * 100
            mse = mean_squared_error(actual_values, ses_forecast)

            # error
            # print(f"Root Mean Squared Error (RMSE): {rmse}")
            # print(f"Mean Absolute Percentage Error (MAPE): {mape}")
            # print(f"Mean Squared Error (MSE): {mse}")

            section.add_content("SES Model",f"RMSE: {rmse}, MAPE: {mape}, MSE: {mse}")

            # plotting SES and forecasted values
            plt.figure(figsize=(12, 6))
            plt.plot(data2.index, data2[data_column], label='Actual Data')
            plt.plot(ses_fit.fittedvalues.index, ses_fit.fittedvalues, label=f'SES (alpha={alpha})')
            plt.plot(ses_forecast.index, ses_forecast, linestyle='--', color='orange', label='Forecasted Values')
            plt.title(f'Forecasting {data_column} Using Simple Exponential Smoothing (SES)')
            plt.xlabel('Date')
            plt.ylabel('Value')
            plt.legend()
            plt.grid(True)

            plt.savefig(f'{plots_folder}/{base_filename}_plot46.png')
            section.add_content("SES","SES with smoothning factor 0.2 and 20 timeperiod forecast",image_path=f'{plots_folder}/{base_filename}_plot46.png')
            section.add_content("SES",f"Forecasted output: Carbon Emissions per {timeperiod_setting}: \n")
            section.add_content("SES",pd.DataFrame(ses_forecast))
            section.add_content("SES",f"Forecasted output saved: {output_folder}/{base_filename}-SES_Predicted_Values.xlsx")

            plt.show()

            # Save the forecasted values
            pd.DataFrame(ses_forecast, columns=[f'{base_filename} SES Predicted Values']).to_excel(f'{output_folder}/{base_filename}-SES_Predicted_Values.xlsx', index=True)

            data_column = f'kgCO2_per_watt_{timeperiod_setting}_{location_country}'

            # double exp/holts linear since there is trend
            model_holt = sm.tsa.Holt(data2[data_column]).fit()

            # forecasting next 10 time periods
            forecast_steps = 10

            # create time line for next forecast
            forecast_index = pd.date_range(start=data2.index[-1], periods=forecast_steps + 1, freq='D')[1:]  # Start from the day after the last date

            # forecastd values
            holt_forecast = model_holt.forecast(steps=forecast_steps)

            # actual values
            actual_values = data2[data_column].iloc[-forecast_steps:]

            # error compute
            mae = mean_absolute_error(actual_values, holt_forecast)
            mse = mean_squared_error(actual_values, holt_forecast)
            rmse = np.sqrt(mse)

            # errors
            # print(f"Mean Absolute Error (MAE): {mae}")
            # print(f"Mean Squared Error (MSE): {mse}")
            # print(f"Root Mean Squared Error (RMSE): {rmse}")

            section.add_content("Holts Linear Model",f"RMSE: {rmse}, MAPE: {mape}, MSE: {mse}")

            # differences between forecasted and actual values
            forecast_difference = holt_forecast - actual_values.values

            # Plotting
            plt.figure(figsize=(12, 6))
            plt.plot(data2.index, data2[data_column], label='Actual Data')
            plt.plot(forecast_index, holt_forecast, linestyle='--', color='orange', label=f'Holt Linear Forecast ({forecast_steps} {timeperiod_setting})')
            plt.title('Double Exponential Holt Linear Forecast')
            plt.xlabel('Date')
            plt.ylabel('Value')
            plt.legend()
            plt.grid(True)

            plt.savefig(f'{plots_folder}/{base_filename}_plot47.png')
            section.add_content("Holts Linear","Holts Linear 10 timeperiod forecast",image_path=f'{plots_folder}/{base_filename}_plot47.png')
            section.add_content("Holts Linear",model_holt.summary(),is_model_summary=True)
            section.add_content("Holts Linear",f"Forecasted output: Carbon Emissions per {timeperiod_setting}: \n")
            section.add_content("Holts Linear",pd.DataFrame(holt_forecast))
            section.add_content("Holts Linear",f"Forecasted output saved: {directory}/{base_filename}-Holts_Predicted_Values.xlsx")

            plt.show()

            # summary and forecast
            # print(model_holt.summary())
            # print(f"Holt Linear Forecast for the next {forecast_steps} periods:")
            # print(holt_forecast)

            # Print differences
            # print("\nForecasted - Actual Differences:")
            # print(forecast_difference)

            # Save the forecasted values
            pd.DataFrame(holt_forecast, columns=[f'{base_filename} Holts Predicted Values']).to_excel(f'{output_folder}/{base_filename}-Holts_Predicted_Values.xlsx', index=True)

            """## ARIMA section"""

            # Train-test split
            train_size = int(len(y) * 0.8)
            X_train, X_test = X[:train_size], X[train_size:]
            y_train, y_test = y[:train_size], y[train_size:]

            section.add_content("Auto ARIMA model ", "Train Test split 80-20 with 5 timeseries split cross validation")

            # auto_arima model
            model_arima = auto_arima(y_train, exogenous=X_train, seasonal=False, trace=True, error_action='ignore', suppress_warnings=True)

            section.add_content("Auto ARIMA model",model_arima.summary(),is_model_summary=True)

            # Summary of the model
            # print(model_arima.summary())

            # Cross-validation
            tscv = TimeSeriesSplit(n_splits=5)
            cv_scores = []
            for train_index, test_index in tscv.split(y):
                X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]
                y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]

                model_cv = auto_arima(y_train_cv, exogenous=X_train_cv, seasonal=False, trace=False, error_action='ignore', suppress_warnings=True)
                forecast_cv = model_cv.predict(n_periods=len(y_test_cv), exogenous=X_test_cv)
                cv_score = smape(y_test_cv, forecast_cv)
                cv_scores.append(cv_score)

            # print(f"Cross-validation scores: {cv_scores}")
            # print(f"Mean cross-validation score: {np.mean(cv_scores)}")

            section.add_content("Auto ARIMA model",f"Cross-validation scores: {cv_scores}")
            section.add_content("Auto ARIMA model",f"Mean cross-validation score: {np.mean(cv_scores)}")

            # fitting the model on the full training data
            model_arima.fit(y_train, exogenous=X_train)

            # forecasting
            n_periods = len(y_test)
            forecast, conf_int = model_arima.predict(n_periods=n_periods, exogenous=X_test, return_conf_int=True)

            # combining forecast and confidence intervals
            forecast_df = pd.DataFrame(forecast, index=y_test.index, columns=['forecast'])
            conf_int_df = pd.DataFrame(conf_int, index=y_test.index, columns=['lower_conf', 'upper_conf'])

            # combining actual and forecast data for comparison
            result = pd.concat([y_test, forecast_df, conf_int_df], axis=1)

            # results
            # print(result)

            # plotting

            section.add_content("Auto Arima Model",f"Forecasted output: Carbon Emissions per {timeperiod_setting}: \n")
            section.add_content("Auto Arima Model",pd.concat([forecast_df,conf_int_df]))
            section.add_content("Auto Arima Model",f"Forecasted output saved: {output_folder}/{base_filename}-A_ARIMA_Predicted_Values.xlsx")

            plt.figure(figsize=(14, 7))
            plt.plot(y_train.index, y_train, label='Training Data')
            plt.plot(y_test.index, y_test, label='Actual Data')
            plt.plot(forecast_df.index, forecast_df, label='Forecasted Data')
            plt.fill_between(conf_int_df.index, conf_int_df['lower_conf'], conf_int_df['upper_conf'], color='k', alpha=0.1, label='Confidence Interval')
            plt.title('ARIMA Forecast')
            plt.xlabel('Date')
            plt.ylabel('Value')
            plt.legend()

            plt.savefig(f'{plots_folder}/{base_filename}_plot48.png')
            section.add_content("Auto Arima Model","Auto Arima Model Graph",image_path=f'{plots_folder}/{base_filename}_plot48.png')

            plt.show()

            # Saving the forecasted values
            pd.DataFrame(pd.concat([forecast_df,conf_int_df]), columns=[f'{base_filename} Auto-ARIMA Predicted Values']).to_excel(f'{output_folder}/{base_filename}-A_ARIMA_Predicted_Values.xlsx', index=True)

            #Check1-okay
            X.index = X.index.to_period('H').to_timestamp()
            y.index = y.index.to_period('H').to_timestamp()

            # Train-test split
            train_size = int(len(y) * 0.8)
            X_train, X_test = X[:train_size], X[train_size:]
            y_train, y_test = y[:train_size], y[train_size:]

            section.add_content("Manual ARIMA model ", "Train Test split 80-20 with 5 timeseries split cross validation p,q,d config 1,1,2")

            # ARIMA model configuration
            order = (1, 1, 0)

            # Manual ARIMA model
            model_arima2 = ARIMA(y_train, order=order, exog=X_train).fit()

            # model summary
            # print(model_arima2.summary())

            section.add_content("Manual ARIMA model",model_arima2.summary(),is_model_summary=True)

            # cross-validation
            tscv = TimeSeriesSplit(n_splits=5)
            cv_scores = []

            for train_index, test_index in tscv.split(y):
                X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]
                y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]

                model_cv2 = ARIMA(y_train_cv, order=order, exog=X_train_cv).fit()
                forecast_cv = model_cv2.predict(start=len(y_train_cv), end=len(y_train_cv) + len(y_test_cv) - 1, exog=X_test_cv)
                cv_score = smape(y_test_cv, forecast_cv)
                cv_scores.append(cv_score)

            # print(f"\n Cross-validation SMAPE scores: {cv_scores}")
            # print(f"\n Mean cross-validation SMAPE score: {np.mean(cv_scores)}")

            section.add_content("Manual ARIMA model",f"Cross-validation scores: {cv_scores}")
            section.add_content("Manual ARIMA model",f"Mean cross-validation score: {np.mean(cv_scores)}")

            # forecast
            n_periods = len(y_test)
            forecast = model_arima2.predict(start=len(y_train), end=len(y_train) + n_periods - 1, exog=X_test)
            conf_int = model_arima2.get_forecast(steps=n_periods, exog=X_test).conf_int()

            # combining forecast and confidence intervals
            forecast_df = pd.DataFrame(forecast, index=y_test.index, columns=['forecast'])
            conf_int_df = pd.DataFrame(conf_int, index=y_test.index, columns=['lower_conf', 'upper_conf'])

            # combining actual and forecast data for comparison
            result = pd.concat([y_test, forecast_df, conf_int_df], axis=1)

            # results
            # print(result)

            # plotting

            section.add_content("Manual Arima Model",f"Forecasted output: Carbon Emissions per {timeperiod_setting}: \n")
            # section.add_content("Manual Arima Model",pd.DataFrame(pd.concat([forecast_df,conf_int_df])))
            # section.add_content("Manual Arima Model",f"Forecasted output saved: {directory}/{base_filename}-M_ARIMA_Predicted_Values.xlsx")

            plt.figure(figsize=(14, 7))
            plt.plot(y_train.index, y_train, label='Training Data')
            plt.plot(y_test.index, y_test, label='Actual Data')
            plt.plot(forecast_df.index, forecast_df, label='Forecasted Data')
            plt.fill_between(conf_int_df.index, conf_int_df['lower_conf'], conf_int_df['upper_conf'], color='k', alpha=0.1, label='Confidence Interval')
            plt.title('ARIMA Forecast')
            plt.xlabel('Date')
            plt.ylabel('Value')
            plt.legend()

            plt.savefig(f'{plots_folder}/{base_filename}_plot49.png')
            section.add_content("Manual Arima Model","Manual Arima Model Graph",image_path=f'{plots_folder}/{base_filename}_plot49.png')
            plt.show()

            # Save the forecasted values
            # pd.DataFrame(pd.concat([forecast_df,conf_int_df]), columns=[f'{base_filename} ARIMA(1,1,2) Predicted Values']).to_excel(f'{directory}/{base_filename}-M_ARIMA_Predicted_Values.xlsx', index=True)

            # normalising the data
            scaler_X = MinMaxScaler(feature_range=(0, 1))
            X_scaled = scaler_X.fit_transform(X)
            scaler_y = MinMaxScaler(feature_range=(0, 1))
            y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))

            # Train-test split-80-20
            train_size = int(len(X_scaled) * 0.8)
            X_train1, X_test1 = X_scaled[:train_size], X_scaled[train_size:]
            y_train1, y_test1 = y_scaled[:train_size], y_scaled[train_size:]

            section.add_content("LSTM model ", "Train Test split 80-20 with values normalised for equal weight distribution")

            # reshape input layer to [1,1,4] config [samples, time steps, features]
            # time steps = 1
            X_train1 = X_train1.reshape((X_train1.shape[0], 1, X_train1.shape[1]))
            X_test1 = X_test1.reshape((X_test1.shape[0], 1, X_test1.shape[1]))

            # LSTM model
            model = Sequential()
            model.add(LSTM(50, input_shape=(X_train1.shape[1], X_train1.shape[2])))
            model.add(Dense(1))  # Output layer with one unit for the target variable

            model.compile(loss='mean_squared_error', optimizer='adam')
            # print(model.summary())
            # section.add_content("LSTM model",model.summary(),is_model_summary=True)

            # fittin the model
            history = model.fit(X_train1, y_train1, epochs=50, batch_size=1, validation_data=(X_test1, y_test1), verbose=2, shuffle=False)

            # predictions
            y_pred1 = model.predict(X_test1)

            # non normalising scaling for prediction
            y_pred_inv = scaler_y.inverse_transform(y_pred1)
            y_test_inv = scaler_y.inverse_transform(y_test1)

            # final model evaluation

            mse = mean_squared_error(y_test_inv, y_pred_inv)
            # print(f'Test Mean Squared Error: {mse}')

            section.add_content("LSTM model",f'Test Mean Squared Error: {mse}')

            # plotting
            plt.figure(figsize=(12, 6))
            plt.plot(y.index[-y_pred1.shape[0]:],y_test_inv, label='Actual')
            plt.plot(y.index[-y_pred1.shape[0]:],y_pred_inv, label='Predicted')
            plt.title('LSTM Forecast')
            plt.xlabel('Time')
            plt.ylabel('Value')
            plt.legend()

            plt.savefig(f'{plots_folder}/{base_filename}_plot50.png')
            section.add_content("LSTM Model","LSTM Model Graph",image_path=f'{plots_folder}/{base_filename}_plot50.png')

            plt.show()


            return section

        """# Model initialisation"""

        # Data with High Correlation- data, split into X,y
        # Data with lag values- data_lag,X_lag,y_lag
        # Data with pca values- data_pca_2,X_pca,y_pca
        # Data with pca-lag values- data_pca_lag,X_pca_lag, y_pca_lag

        section6 = PDFSection("Exploring Models")

        data2=data.copy(deep=True)
        y = data2[f'kgCO2_per_watt_{timeperiod_setting}_{location_country}']
        X = data2.drop(columns=f'kgCO2_per_watt_{timeperiod_setting}_{location_country}')

        section7 = PDFSection("Untreated Data Models")
        section7=model_analysis(X,y,section7,timeperiod_setting,plots_folder1,output_folder1,base_filename,location_country)

        section8 = PDFSection("Lagged Data Models")
        section8=model_analysis(X_lag,y_lag,section8,timeperiod_setting,plots_folder2,output_folder2,base_filename,location_country)

        section9 = PDFSection("PCA Data Models")
        section9=model_analysis(X_pca,y_pca,section9,timeperiod_setting,plots_folder3,output_folder3,base_filename,location_country)

        section10 = PDFSection("PCA+Lagged Data Models")
        section10=model_analysis(X_pca_lag,y_pca_lag,section10,timeperiod_setting,plots_folder4,output_folder4,base_filename,location_country)

        create_pdf(f"{directory}/{base_filename}_complete_analysis.pdf", f"{base_filename} Complete Data Analysis and Modelling Report", banner_path, [section1, section2,section3,section4,section5,section6,section7,section8,section9,section10])

directory_path="/content/drive/MyDrive/Dissertation/Data_revised/Final"
csv_files = glob.glob(os.path.join(directory_path, '*.csv'))
# Process each file
for file_path in csv_files:
    complete_analysis_function(file_path)

