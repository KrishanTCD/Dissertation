# -*- coding: utf-8 -*-
"""Data-Loading

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-cp4y3yJ66WHVgNTz7h4dWsTG8HjxsHX
"""

# Print gpu info
gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

# Check for more memory
from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

from google.colab import drive
drive.mount('/content/drive')

import os
import gzip
import pandas as pd
import datetime

def preprocess_file(file_path):
    # Read the file into a DataFrame
    with gzip.open(file_path) as f:
        df = pd.read_csv(f, delimiter='\s+')

    # Primary preprocessing
    df = primary_preprocess_data(df)

    # Secondary preprocessing
    df = secondary_preprocess_data(df)

    return df

def primary_preprocess_data(df):
    # count missing values
    #df.info(verbose=True)

    missing_values = df.isnull().sum()

    # check duplicate values
    duplicate_values = df.duplicated().sum()
    print(f"\n Misssing values: {missing_values} \n Duplicate values: {duplicate_values} \n")

    # Highlight duplicated values
    # duplicates = df[df.duplicated()]
    # print(duplicates)

    df.drop_duplicates(inplace=True)

    # check for duplicates again
    duplicate_values = df.duplicated().sum()
    print(f"Check 2 for duplicated values: {duplicate_values} \n")

    return df

def secondary_preprocess_data(df):
    # Fill null values in PkgWatt with forward fill method and other - values to ensure drop doesnt remove them all
    df['PkgWatt'].fillna(method='ffill', inplace=True)
    df['CPU'].replace('-', '0', inplace=True)
    df['Core'].replace('-', '0', inplace=True)

    # Check for non-numeric values and print them, then drop rows containing them
    # Otherwise we wont be able to convert datatypes
    for column in df.columns:
          non_numeric_rows = df[pd.to_numeric(df[column], errors='coerce').isna()]
          if not non_numeric_rows.empty:
              print(f"Non-numeric values found in column {column}:\n{non_numeric_rows}")
          df = df[pd.to_numeric(df[column], errors='coerce').notna()]

    # Convert Time_Of_Day_Seconds from Unix time (nanoseconds) to a readable datetime format
    df['Time_Of_Day_Seconds'] = df['Time_Of_Day_Seconds'].astype(float).apply(lambda x: datetime.datetime.fromtimestamp(x))

    #Convert data type of the rest of the columns to float
    for column in df.columns:
        if column != 'Time_Of_Day_Seconds':
            df[column] = df[column].astype(float)
    return df

def combine_and_preprocess_data(directory_path):
    all_combined_data = []

    # Iterate over each file in the directory
    for filename in os.listdir(directory_path):
        if filename.endswith('.gz'):
            file_path = os.path.join(directory_path, filename)
            print(f"Processing file: {file_path}")

            # Preprocess the file
            df = preprocess_file(file_path)

            # Aggregate at the 'seconds' level using mean
            # Round Time_Of_Day_Seconds to the nearest second
            df['Time_Of_Day_Seconds'] = df['Time_Of_Day_Seconds'].dt.floor('S')
            agg_dict = {col: 'mean' for col in df.columns if col != 'PkgWatt' and col != 'Time_Of_Day_Seconds'}
            agg_dict['PkgWatt'] = 'sum'
            # Group by Time_Of_Day_Seconds and calculate the mean of other columns
            df_aggregated = df.groupby('Time_Of_Day_Seconds').agg(agg_dict).reset_index()

            # Append to list
            all_combined_data.append(df_aggregated)

    # Concatenate all dataframes into one
    combined_df = pd.concat(all_combined_data, ignore_index=True)

    # Sort by timestamp or datetime column if needed
    combined_df.sort_values(by='Time_Of_Day_Seconds', inplace=True)
    combined_df.reset_index(drop=True, inplace=True)

    return combined_df

def create_processed_folders(main_directory, processed_directory):
    # Iterate over each folder in the main directory
    for folder_name in os.listdir(main_directory):
        folder_path = os.path.join(main_directory, folder_name)

        # Check if it's a directory
        if os.path.isdir(folder_path):
            print(f"Processing folder: {folder_name}")

            # Combine and preprocess data for this folder
            combined_data = combine_and_preprocess_data(folder_path)

            # Create new folder in processed_directory
            new_folder_path = os.path.join(processed_directory, folder_name)
            os.makedirs(new_folder_path, exist_ok=True)

            # Save combined data as CSV in new folder
            output_file_path = os.path.join(new_folder_path, f"{folder_name}_combined.csv")
            combined_data.to_csv(output_file_path, index=False)

            print(f"Processed data saved to: {output_file_path}")

# Main directories
main_directory = '/content/drive/MyDrive/Dissertation/Data_revised/Extracted_Data'
processed_directory = '/content/drive/MyDrive/Dissertation/Data_revised/Processed_Data2'

# Create processed folders and save combined data with mean aggregation
create_processed_folders(main_directory, processed_directory)



















